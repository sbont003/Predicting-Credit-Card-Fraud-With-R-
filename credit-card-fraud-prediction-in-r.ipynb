{"cells":[{"metadata":{"_uuid":"becb158c2416465267d5966180ebdc0cd8ec2815","_cell_guid":"69cb9881-75a8-48b2-a5c6-6dd15be8bcd1"},"cell_type":"markdown","source":"# Predicting fraudulent credit card transactions\n\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f8ea5b4caedcb879da72d6f36ee9e48126f3bdc7","_cell_guid":"8b763580-cdf0-47ce-a928-fabaddb8a15e"},"cell_type":"markdown","source":"According to [creditcards.com][1], there was over £300m in fraudulent credit card transactions in the UK in the first half of 2016, with banks preventing over £470m of fraud in the same period. The data shows that credit card fraud is rising, so there is an urgent need to continue to develop new, and improve current, fraud detection methods.\n\nUsing this dataset, we will use machine learning to develop a model that attempts to predict whether or not a transaction is fraudlent. To preserve anonymity, these data have been transformed using principal components analysis.\n\nTo begin this analysis, we will first train a random forest model to establish a benchmark, before looping back to EDA, looking at the most important predictive variables and testing other models.\n[1]: http://uk.creditcards.com/credit-card-news/uk-britain-credit-debit-card-statistics-international.php","outputs":[],"execution_count":null},{"metadata":{"_kg_hide-output":true,"_uuid":"26e75eb0ad297b6cad9668d3c3335cfa4b6c8a7d","trusted":true,"_cell_guid":"98d09a5a-1f40-494c-b13a-43ca4c0e3a94"},"cell_type":"code","source":"# load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(ggplot2)\nlibrary(Hmisc)\nlibrary(party)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8b06f53cab38a3b8bfacb2f88054d0b209f3c3a","trusted":true,"_cell_guid":"2b82512e-cfe8-4941-9b84-8e605de99199"},"cell_type":"code","source":"# set random seed for model reproducibility\nset.seed(1234)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"dd9cefec36f9e1b21d9d831938ab86756746638f","trusted":true,"_cell_guid":"d4a65aa6-b8de-410c-88bd-63526a79d93a"},"cell_type":"code","source":"# import data\ncreditData <- read_csv(\"../input/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4febc254dee5f204ad15ff579fc3ea736dd0d95","trusted":true,"_cell_guid":"85efc637-9913-4431-9b27-c6d1b5153555"},"cell_type":"code","source":"# look at the data\nglimpse(creditData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165a9e29a91a9ed3f9ab0f7d30b45946ca0cd238","trusted":true,"_cell_guid":"c9a28944-54e3-4bb2-8899-eab641eb7c63"},"cell_type":"code","source":"# make Class a factor\ncreditData$Class <- factor(creditData$Class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ad57bf3fd2a6541d3706eb894c5e3c12a50c0a","trusted":true,"_cell_guid":"2991445f-335e-4dc8-af7c-9861a1b4603b"},"cell_type":"code","source":"train <- creditData[1:150000, ]\ntest <- creditData[150001:284807, ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7982d4ec041263cc85da5db19109ae0f27aed12","trusted":true,"_cell_guid":"5be98750-cf6a-4447-9f97-c08c2d71c446"},"cell_type":"code","source":"train %>%\n  select(Class) %>%\n  group_by(Class) %>%\n  summarise(count = n()) %>%\n  glimpse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84170e1de363b1a727f726cc8172eb5607d79b34","trusted":true,"_cell_guid":"a37d02f3-0d98-4aa8-b02d-452459d25c66"},"cell_type":"code","source":"test %>%\n  select(Class) %>%\n  group_by(Class) %>%\n  summarise(count = n()) %>%\n  glimpse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad72e98f83072855929737cd3090e27d735aae22","_cell_guid":"0b777640-5568-4f2e-835b-ecf83be2e975"},"cell_type":"markdown","source":"As we can see, fraudulent transactions are a very small proportion of our dataset, we could build what would appear to be a highly accurate model just by always saying that every transaction was not fraudulent. While we would be right over 99% of the time, that would cost consumers and the industry over £500m per year, so wouldn't be a useful model.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"00c2930f3e7880d687a6599b7fdcd984c28876f4","trusted":true,"_cell_guid":"f1da877b-0d6b-4c0f-ad10-c55b67e1c398"},"cell_type":"code","source":"# build random forest model using every variable\nrfModel <- randomForest(Class ~ . , data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1025c249b5e7d5d1b80f62a383a42dc5e4bde4e","trusted":true,"_cell_guid":"c92b54c7-92b3-4de5-8f89-c0141db2c538"},"cell_type":"code","source":"test$predicted <- predict(rfModel, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2231719fe588acaa7009bacbf013cc1bf743588","_cell_guid":"6df66517-4ab3-478b-910c-a646460bd0fc"},"cell_type":"markdown","source":"So we've built a random forest model using all the available variables and used it to predict whether or not a transaction is fraudulent on our test test. As the data is very imbalanced, we would expect our accuracy to be very high, even if our model just always guessed 'not fraudulent'.\n\n[Jason Brownlee has some useful options to work through when you have an imbalanced dataset][1] but, for now, while it will give us the accuracy, the `confusion matrix` function in the `caret` package does give us some other useful metrics:\n[1]: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3d5c366ae3550bfeed242cdaa3a518cd59504e6a","trusted":true,"_cell_guid":"a317ab31-4f8e-4b44-8e80-d18117d74981"},"cell_type":"code","source":"library(caret)\nconfusionMatrix(test$Class, test$predicted)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9b58f8d9309b26751c0ba79ac505eaf5e7fdf8","_cell_guid":"7a64d505-20dc-428c-b3e4-5e785566468e"},"cell_type":"markdown","source":"Looking at the output above, we can indeed see that we have very high accuracy (99.94%) as we expected.  Going back to our test set class counts, we can see that we had 134,608 legitimate transactions, and 199 fraudulent transactions. Had we said that every transaction was fraudulent, we would have got 199 wrong.\n\nIf we look at our confusion matrix, we can see that, using our model, we only got 82 predictions wrong; this figure of 82 is made up of 47 false positive and 35 false negatives. Going from 199 wrong to 82 wrong is quite an improvement on performance, but we have to consider the imporance of sensitivity and specificity when it comes to the real-world application. Are the implications of a false negative more or less sigificant than the implications of a false positive?\n\nUltimately, there is no way lenders could function if they classified every transaction as fraud and investigated it thoroughly before deciding whether or not to approve it, the costs of doing that would be so high that it wouldn't be feasible. If the lenders let every transaction through, the costs associated with the fraud would escalate.\n\nIn the absence of 100% accuracy, when we are building our models, it is important to consider the purpose of the model and how it will be used. We could optimise our model for area under the ROC curve, but if the real-world use of the model places more importance on reducing false negatives rather than false positives, we may be training against the wrong metric.\n\nFor this model, let's use the F1 score from the `MLmetrics` package.","outputs":[],"execution_count":null},{"metadata":{"_kg_hide-output":true,"_uuid":"d4751d7b50b129c1eb08930acb9fe79b6f2c9b03","trusted":true,"_cell_guid":"2fa30ff3-c554-4a3c-a8bd-1a8868cda26b"},"cell_type":"code","source":"library(MLmetrics)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e6db69cb30583b6262ede18dfe910846f495ec","trusted":true,"_cell_guid":"f219d2e2-b86f-4ace-b6f0-c93e393809de"},"cell_type":"code","source":"F1_all <- F1_Score(test$Class, test$predicted)\nF1_all","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ec47783634ac32943aa4c2f5aeeec5376d80be","_cell_guid":"9edbfa52-0f18-434f-83f3-4090ff9faefd"},"cell_type":"markdown","source":"Now we have our benchmark figure, obtained very quickly using all the variables to train a random forest model with no tuning. Now to see if we can either simplify, without losing accuracy, or improve that score.\n\nTo start off with, let's look at the importance of the predictors:","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9f829edd0ff42c3f25f28c68a2339521c57b482f","trusted":true,"_cell_guid":"e0458f93-d58a-4509-a97c-099d877a805e"},"cell_type":"code","source":"options(repr.plot.width=5, repr.plot.height=4)\nvarImpPlot(rfModel,\n          sort = T,\n           n.var=10,\n           main=\"Top 10 Most Important Variables\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8edbcc9d2a2353a01608283345cf2df563e03b2"},"cell_type":"markdown","source":"Let's see what sort of performance we get with just out top predictive variable:","outputs":[],"execution_count":null},{"metadata":{"_uuid":"236d7d2f0700078b789a6d969933654e3c02cc43","trusted":true,"_cell_guid":"2406cd55-058d-4160-9ec3-196ad014c87b"},"cell_type":"code","source":"rfModelTrim1 <- randomForest(Class ~  V17, \n                            data = train)\n\ntest$predictedTrim1 <- predict(rfModelTrim1, test)\n\nF1_1 <- F1_Score(test$Class, test$predictedTrim1)\nF1_1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35fc8eed67583e1fa4a165870b82d34839185059"},"cell_type":"markdown","source":"Not a bad score at all, and our run time for the train was considerably shorter. What about if we go with the top 2?","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"6d773e4baac233e4f8e15513cc6af8c4c5e437ba"},"cell_type":"code","source":"rfModelTrim2 <- randomForest(Class ~  V17 + V12, \n                            data = train)\n\ntest$predictedTrim2 <- predict(rfModelTrim2, test)\n\nF1_2 <- F1_Score(test$Class, test$predictedTrim2)\nF1_2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f7264288d2ebbda35a1f8116de1c44416f4394d"},"cell_type":"markdown","source":"That takes us up to the 0.9996 level, so we're already getting close to our performance using all the variables, but with much faster run times. What about the top 3?","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"9a1708e2df1fd6b877fb142e6adc0f4b843c94e6"},"cell_type":"code","source":"rfModelTrim3 <- randomForest(Class ~  V17 + V12 + V14, \n                            data = train)\n\ntest$predictedTrim3 <- predict(rfModelTrim3, test)\n\nF1_3 <- F1_Score(test$Class, test$predictedTrim3)\nF1_3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be825eee00623260dd59dad8b7c0e19cd2715e68"},"cell_type":"markdown","source":"A bit of a dip there, but that could just be due to chance. Let's try a few more models of increasing complexity to see what sort of trend emerges. We could do this in a for loop so that we could set it running and go away and have a nice coffee, but I want to get a feel for how long each model takes to run and keep the code simple to follow, so we'll keep things seperate.\n\nI might return to that in a future kernel though, it might be interesting to play with time-stamping the start and end-points of processes and calculating the run time for each iteration through a loop...","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"d4cc11ed47f5a6ac7ad826fcaaf9a0cc7fb6bb97"},"cell_type":"code","source":"# four variables\nrfModelTrim4 <- randomForest(Class ~  V17 + V12 + V14 + V10, \n                            data = train)\n\ntest$predictedTrim4 <- predict(rfModelTrim4, test)\n\nF1_4 <- F1_Score(test$Class, test$predictedTrim4)\nF1_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb385f518dac72499df3b137ba917c12b5e6095f"},"cell_type":"code","source":"# five variables\nrfModelTrim5 <- randomForest(Class ~  V17 + V12 + V14 + V10 + V16, \n                            data = train)\n\ntest$predictedTrim5 <- predict(rfModelTrim5, test)\n\nF1_5 <- F1_Score(test$Class, test$predictedTrim5)\nF1_5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e22b818a43201d599c272bb2a1f76549c2aca0e"},"cell_type":"code","source":"# ten variables\nrfModelTrim10 <- randomForest(Class ~  V17 + V12 + V14 + V10 + V16 \n                              + V11 + V9 + V4 + V18 + V26, \n                            data = train)\n\ntest$predictedTrim10 <- predict(rfModelTrim10, test)\n\nF1_10 <- F1_Score(test$Class, test$predictedTrim10)\nF1_10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5988620ed1c5dd45532833d81bf8aa1219f066ab"},"cell_type":"markdown","source":"With those scores calculated, let's go ahead and plot those out:","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"3cfc452e13da2737102e5f9de1e6488ac8e3c2e3"},"cell_type":"code","source":"# build dataframe of number of variables and scores\nnumVariables <- c(1,2,3,4,5,10,17)\nF1_Score <- c(F1_1, F1_2, F1_3, F1_4, F1_5, F1_10, F1_all)\nvariablePerf <- data.frame(numVariables, F1_Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60e015c5ed7a590e6d165a4b16f6d057ed784737"},"cell_type":"code","source":"# plot score performance against number of variables\noptions(repr.plot.width=4, repr.plot.height=3)\nggplot(variablePerf, aes(numVariables, F1_Score)) + geom_point() + labs(x = \"Number of Variables\", y = \"F1 Score\", title = \"F1 Score Performance\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4378bcb10ae009d99418dec3f29134662b9411cf"},"cell_type":"code","source":"rf10 = randomForest(Class ~  V17 + V12 + V14 + V10 + V16 \n                              + V11 + V9 + V4 + V18 + V26,  \n                   ntree = 1000,\n                   data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"076e20a94fef539ed681593a44e1fc596ff6a459"},"cell_type":"code","source":"options(repr.plot.width=6, repr.plot.height=4)\nplot(rf10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b39ab9bdc7861347fe81c0f1f6f86ae4d108183"},"cell_type":"markdown","source":"Plotting our 10-variable model shows that there is not much additional performance gained after what looks like about 50 trees, but let's zoom in on that region to make sure:","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"b965b63cbc7ae627888dee30e7487cea3a544bfd"},"cell_type":"code","source":"options(repr.plot.width=6, repr.plot.height=4)\nplot(rf10, xlim=c(0,100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2713577e52325ecd861aaffcddf529814844579b"},"cell_type":"markdown","source":"## Summing up\nWe have used a random forest method to predict whether or not a credit card transaction is fraudulent or not, and built a model that offers a useful uplift over the no information rate. By testing models using an increasing number of variables, we have begun to explore the balance between model performance and run-time. \n\nThis basic model provides a starting point for contintinuing to tune the model to seek additional improvements. Although we are dealing with changes in accuracy at the fourth decimal place, these very slight changes in accuracy need to be considered with respect to the volume of credit card transactions that take place every year. Even with only a slight fraction of these being fraudulent, the sheer volume of transactions mean that even very slight improvements in model performance may result in significant reductions in credit card fraud.","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}